{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8667af5d-0ffa-44d3-a137-01c34b8be637",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e42734-3f2e-4637-9c4f-fed313226fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For installing pytorch, use:\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbee16d5-edb7-446c-8e22-a6a4a36f08a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import tifffile as tiff\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccdfa0f3-56db-4d44-a140-fd314137067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "csv_file = \"../metadata.csv\"\n",
    "image_folder = \"../IMC_images\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Filter rows with NA in PDL1_score and convert to binary\n",
    "df = df.dropna(subset=[\"PDL1_score\"])\n",
    "df[\"PDL1_score\"] = df[\"PDL1_score\"].astype(int)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Train-test split\n",
    "train_df, val_and_test_df = train_test_split(df, test_size=0.4, random_state=42, stratify=df[\"PDL1_score\"])\n",
    "test_df, val_df = train_test_split(val_and_test_df, test_size=0.5, random_state=42, stratify=val_and_test_df[\"PDL1_score\"])\n",
    "\n",
    "class IMCDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['sample_id']}.tiff\")\n",
    "        \n",
    "        # Load the multi-channel tiff image with tifffile\n",
    "        image = tiff.imread(image_path)  # This will load all 46 channels\n",
    "\n",
    "        # Convert to a PyTorch tensor\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Ensure the shape is (channels, height, width)\n",
    "        if image.shape[0] != 46:\n",
    "            raise ValueError(f\"Expected 46 channels, but got {image.shape[0]} in {image_path}\")\n",
    "\n",
    "        # Apply transformations if defined\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(row[\"PDL1_score\"], dtype=torch.long)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873de8b7-f53f-45c6-8e10-03ac696d4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to accumulate sum and sum of squares\n",
    "nr_images = 0\n",
    "sum_images = torch.zeros((46, 224, 224))  # for 46 channels, assuming the image size is 224x224\n",
    "sum_squared_images = torch.zeros((46, 224, 224))  # to accumulate squared pixel values\n",
    "\n",
    "# Assuming unnormalized_train_dataset is an instance of IMCDataset\n",
    "unnormalized_train_dataset = IMCDataset(train_df, image_folder)\n",
    "\n",
    "# Loop through the dataset to accumulate the sum and sum of squared pixel values\n",
    "for image, _ in unnormalized_train_dataset:\n",
    "    nr_images += 1\n",
    "    sum_images += image\n",
    "    sum_squared_images += image ** 2\n",
    "\n",
    "# Compute the mean and standard deviation\n",
    "mean = sum_images / nr_images\n",
    "std = torch.sqrt(sum_squared_images / nr_images - mean ** 2)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "data_augmentation_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = IMCDataset(train_df, image_folder, transform=transform)\n",
    "train_data_augmentation_dataset = IMCDataset(train_df, image_folder, transform=data_augmentation_transform)\n",
    "val_dataset = IMCDataset(val_df, image_folder, transform=transform)\n",
    "test_dataset = IMCDataset(test_df, image_folder, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "train_data_augmentation_loader = DataLoader(train_data_augmentation_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac441b14-a7d5-4248-bd48-c1c2eaf514be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old model:\n",
    "# # Define the model\n",
    "# class SimpleCNN(nn.Module):\n",
    "#     def __init__(self, num_channels=46, num_classes=2):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.conv1a = nn.Conv2d(num_channels, 32, kernel_size=3, padding=1)\n",
    "#         self.conv1b = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(32)\n",
    "#         self.conv1c = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(32)\n",
    "#         self.conv1d = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(32, 20, kernel_size=3, padding=1)\n",
    "#         self.fc1 = nn.Linear(20 * 56 * 56, 50)\n",
    "#         self.fc2 = nn.Linear(50, num_classes)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(self.relu(self.conv1a(x)))\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(self.conv1b(x))\n",
    "#         x = self.relu(self.conv1c(x))\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu(self.conv1d(x))\n",
    "#         x = self.pool(self.relu(self.conv2(x)))\n",
    "#         x = x.view(x.size(0), -1)  # Flatten\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af917124-3725-4e6d-9e7f-0c765afb1291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-luvogt/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jupyter-luvogt/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class ModifiedResNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, input_channels=46):\n",
    "        super(ModifiedResNet, self).__init__()\n",
    "\n",
    "        # Load the pretrained ResNet18 model\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Modify the first convolution layer to accept 46 input channels\n",
    "        # The original model has 3 input channels (RGB), so we replace it with a layer that accepts 46 channels\n",
    "        self.model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        # Modify the final fully connected layer to output 2 classes\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Create an instance of the modified model\n",
    "model = ModifiedResNet(num_classes=2, input_channels=46).to(device)\n",
    "\n",
    "# for param in model.model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Unfreeze the layers you want to train\n",
    "# model.model.conv1.requires_grad = True\n",
    "# model.model.fc.requires_grad = True\n",
    "\n",
    "# Initialize loss, optimizer\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df[\"PDL1_score\"]), y=train_df[\"PDL1_score\"])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Used for knowning when to save the model\n",
    "best_validation_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a33d7c89-18e5-43ec-b082-c6ed8a9f9f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.09168258290737867, accuracy: 97.97%\n",
      "Validation Accuracy: 76.65%\n",
      "New best accuracy found.\n",
      "Epoch 2, Loss: 0.03615222326479852, accuracy: 99.15%\n",
      "Validation Accuracy: 69.54%\n",
      "Epoch 3, Loss: 0.013712524593574926, accuracy: 100.00%\n",
      "Validation Accuracy: 78.17%\n",
      "New best accuracy found.\n",
      "Epoch 4, Loss: 0.0043061112199211495, accuracy: 100.00%\n",
      "Validation Accuracy: 77.16%\n",
      "Epoch 5, Loss: 0.0015587147732730954, accuracy: 100.00%\n",
      "Validation Accuracy: 76.14%\n",
      "Epoch 6, Loss: 0.0018720589810982346, accuracy: 100.00%\n",
      "Validation Accuracy: 76.65%\n",
      "Epoch 7, Loss: 0.011632919844123535, accuracy: 99.66%\n",
      "Validation Accuracy: 75.63%\n",
      "Epoch 8, Loss: 0.006649886607192457, accuracy: 99.83%\n",
      "Validation Accuracy: 75.63%\n",
      "Epoch 9, Loss: 0.004007341220858507, accuracy: 99.83%\n",
      "Validation Accuracy: 74.62%\n",
      "Ran out of patience, stopping.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, max_patience=5):\n",
    "    global best_validation_accuracy\n",
    "    # Stop when patience reaches max_patience\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        if val_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = val_accuracy\n",
    "            torch.save(model, 'best_model.pth')\n",
    "            print(\"New best accuracy found.\")\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > max_patience:\n",
    "                print(\"Ran out of patience, stopping.\")\n",
    "                break\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37131aaf-8346-4563-aa8e-4a0de672b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_671214/1789427521.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"best_model.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 78.68%\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model = torch.load(\"best_model.pth\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a28436-e679-43ad-8df5-fe9f49221cde",
   "metadata": {},
   "source": [
    "## Trying with autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90301387-ae37-4199-9fd6-524fcd3badb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "224 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3878d276-90a2-481b-8e43-3897d94340e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(46, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=100352, out_features=1024, bias=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=100352, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Unflatten(dim=1, unflattened_size=(128, 28, 28))\n",
       "    (3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose2d(64, 46, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, num_channels=46):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 64, kernel_size=3, stride=2, padding=1),  # Conv layer with downsampling\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # More downsampling\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),  # Even more downsampling\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 28 * 28, 1024),  # Flatten and reduce to a bottleneck dimension\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1024, 128 * 28 * 28),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (128, 28, 28)),  # Reshape to image format\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, num_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x_reconstructed = self.decoder(x)\n",
    "        return x_reconstructed\n",
    "\n",
    "# Instantiate the autoencoder model\n",
    "autoencoder = Autoencoder(num_channels=46)\n",
    "autoencoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e335b193-ae6c-407a-8cc1-9b89379ea477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Autoencoder Loss: 237.12313725398138\n",
      "Epoch 2/10, Autoencoder Loss: 235.27093857985275\n",
      "Epoch 3/10, Autoencoder Loss: 234.76539377065805\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Autoencoder Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train the autoencoder first\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(model, dataloader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 26\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Autoencoder Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Optimizer for the autoencoder\n",
    "autoencoder_optimizer = optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss function for reconstruction (Mean Squared Error)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Training the autoencoder\n",
    "def train_autoencoder(model, dataloader, optimizer, criterion, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed = model(images)\n",
    "            loss = criterion(reconstructed, images)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Autoencoder Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# Train the autoencoder first\n",
    "train_autoencoder(autoencoder, train_loader, autoencoder_optimizer, mse_loss, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a789f384-eca2-4a74-878c-639e707ca06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, bottleneck_size=1024, num_classes=2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(bottleneck_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Freeze the autoencoder to use its encoder part for feature extraction\n",
    "autoencoder.eval()  # Set to evaluation mode\n",
    "\n",
    "# Create the classifier\n",
    "classifier = Classifier(bottleneck_size=1024, num_classes=2)\n",
    "classifier.to(device)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df[\"PDL1_score\"]), y=train_df[\"PDL1_score\"])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e569a821-8b9c-4970-9e62-13c1f50ce0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Classifier Loss: 1.2155511746039758, Accuracy: 49.94%\n",
      "Validation Accuracy: 57.36%\n",
      "Epoch 2/10, Classifier Loss: 0.7878441031162555, Accuracy: 50.44%\n",
      "Validation Accuracy: 57.36%\n",
      "Epoch 3/10, Classifier Loss: 0.7239592671394348, Accuracy: 49.56%\n",
      "Validation Accuracy: 57.36%\n",
      "Epoch 4/10, Classifier Loss: 0.7189832283900335, Accuracy: 45.62%\n",
      "Validation Accuracy: 57.36%\n",
      "Epoch 5/10, Classifier Loss: 0.7058533384249761, Accuracy: 51.46%\n",
      "Validation Accuracy: 57.36%\n",
      "Epoch 6/10, Classifier Loss: 0.7180602917304406, Accuracy: 46.76%\n",
      "Validation Accuracy: 57.36%\n",
      "Epoch 7/10, Classifier Loss: 0.7160849158580487, Accuracy: 46.25%\n",
      "Validation Accuracy: 57.36%\n",
      "Epoch 8/10, Classifier Loss: 0.7228931876329275, Accuracy: 46.25%\n",
      "Validation Accuracy: 57.36%\n",
      "Epoch 9/10, Classifier Loss: 0.7259556834514325, Accuracy: 46.25%\n",
      "Validation Accuracy: 57.36%\n",
      "Epoch 10/10, Classifier Loss: 0.7279959550270667, Accuracy: 46.25%\n",
      "Validation Accuracy: 57.36%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimizer for the classifier\n",
    "classifier_optimizer = optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop for the classifier\n",
    "def train_classifier(classifier, dataloader, val_loader, optimizer, criterion, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        classifier.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Extract features from the autoencoder's encoder\n",
    "            with torch.no_grad():\n",
    "                features = autoencoder.encoder(images)\n",
    "\n",
    "            # Classifier forward pass\n",
    "            outputs = classifier(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Classifier Loss: {running_loss/len(dataloader)}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "        \n",
    "        # Validation step\n",
    "        classifier.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                 # Extract features from the autoencoder's encoder\n",
    "                with torch.no_grad():\n",
    "                    features = autoencoder.encoder(images)\n",
    "\n",
    "                # Classifier forward pass\n",
    "                outputs = classifier(features)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Train the classifier\n",
    "train_classifier(classifier, train_loader, val_loader, classifier_optimizer, criterion, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c242d-5876-4103-b0d3-35d8842fef79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BME342 Env",
   "language": "python",
   "name": "bme342_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
