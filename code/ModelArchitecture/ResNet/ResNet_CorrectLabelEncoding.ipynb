{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d00de6-d8b1-49e0-a822-0772236e8def",
   "metadata": {},
   "source": [
    "### ResNet18: Tensorflow Pre-Trained\n",
    "\n",
    "Due to difficulty overcoming overfitting with the previous approaches, and probably the data preprocessing being a potential source of error, \n",
    "\n",
    "the whole data importing and preprocessing was revised by first constructing a model with PyTorch and then translating it into tensorflow/keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e295a38-9f17-47a3-a3de-716688e75b7e",
   "metadata": {},
   "source": [
    "### Initialize Notebook & packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931d2ee7-86e4-47e3-a404-20aee57a5b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 21:19:32.962124: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-15 21:19:32.985326: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-15 21:19:32.995121: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 21:19:33.082027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU enabled. Checking for available GPUs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734297579.189522  681772 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "\n",
      "Verifying TensorFlow and PyTorch CUDA setup...\n",
      "TensorFlow version: 2.17.0\n",
      "Built with CUDA: True\n",
      "Num GPUs Available: 1\n",
      "\n",
      "Keras version: 3.6.0\n",
      "\n",
      "End checks and initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734297579.568320  681772 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734297579.572559  681772 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734297579.578241  681772 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734297579.581671  681772 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734297579.584968  681772 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734297579.838354  681772 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734297579.840413  681772 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1734297579.842283  681772 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-15 21:19:39.844102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13312 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:06.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import helper as hp \n",
    "hp.initialize_notebook() # initialize with GPU enabled  \n",
    "# hp.initialize_notebook(False) # to disable GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e505a7a3-0dcf-4cfe-8b7a-9db2438479f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import keras\n",
    "# Import DL libraries\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras import layers, models, Model, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.callbacks import TensorBoard \n",
    "\n",
    "# Suppress tensorflow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Import image libraries\n",
    "from skimage import transform\n",
    "import tifffile as tiff\n",
    "\n",
    "# Appends current working dir\n",
    "current_path = os.getcwd()\n",
    "sys.path.append(current_path)\n",
    "\n",
    "# Import custom preprocessing class\n",
    "from imc_preprocessing import IMCPreprocessor\n",
    "\n",
    "# Import Stratified Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tifffile as tiff\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006ba8a-9e06-4123-91a3-9211efd42ff3",
   "metadata": {},
   "source": [
    "## IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c1bad0-b617-4dd6-a217-5b3386e1672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (590, 224, 224, 46), y_train shape: (590,)\n",
      "X_val shape: (197, 224, 224, 46), y_val shape: (197,)\n",
      "X_test shape: (197, 224, 224, 46), y_test shape: (197,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tifffile import imread\n",
    "images_dir = '/home/jupyter-luvogt/Final_Project_LR/IMC_images' \n",
    "metadata_dir = '/home/jupyter-luvogt/Final_Project_LR/metadata.csv'\n",
    "# --- Load CSV and filter dataset ---\n",
    "csv_file = metadata_dir\n",
    "image_folder = images_dir\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Filter rows with NA in PDL1_score and convert to binary\n",
    "df = df.dropna(subset=[\"PDL1_score\"])\n",
    "df[\"PDL1_score\"] = df[\"PDL1_score\"].astype(int)\n",
    "\n",
    "# --- Train-Test-Validation Split ---\n",
    "train_df, val_and_test_df = train_test_split(df, test_size=0.4, random_state=42, stratify=df[\"PDL1_score\"])\n",
    "test_df, val_df = train_test_split(val_and_test_df, test_size=0.5, random_state=42, stratify=val_and_test_df[\"PDL1_score\"])\n",
    "\n",
    "# --- Load Images and Compute Mean and Standard Deviation ---\n",
    "\n",
    "# Function to load a single image\n",
    "def load_image(image_path):\n",
    "    image = imread(image_path)  # Load all 46 channels\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "# Initialize variables to accumulate sum and sum of squares\n",
    "nr_images = 0\n",
    "sum_images = np.zeros((46, 224, 224))\n",
    "sum_squared_images = np.zeros((46, 224, 224))\n",
    "\n",
    "# Accumulate the sum and sum of squares for the training dataset\n",
    "for _, row in train_df.iterrows():\n",
    "    image_path = os.path.join(image_folder, f\"{row['sample_id']}.tiff\")\n",
    "    image = load_image(image_path)\n",
    "    nr_images += 1\n",
    "    sum_images += image\n",
    "    sum_squared_images += image ** 2\n",
    "\n",
    "# Compute mean and standard deviation\n",
    "mean = sum_images / nr_images\n",
    "std = np.sqrt(sum_squared_images / nr_images - mean ** 2)\n",
    "\n",
    "# --- Dataset Loading Function ---\n",
    "\n",
    "def load_dataset(dataframe, image_folder, augment=False):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in dataframe.iterrows():\n",
    "        image_path = os.path.join(image_folder, f\"{row['sample_id']}.tiff\")\n",
    "        image = load_image(image_path)\n",
    "\n",
    "        # Normalize the image\n",
    "        image = (image - mean) / std\n",
    "\n",
    "        # Convert to channels-last format (224, 224, 46)\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "        # Apply data augmentation if specified\n",
    "        if augment:\n",
    "            # Random horizontal flip\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = np.flip(image, axis=2)\n",
    "            # Random vertical flip\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = np.flip(image, axis=1)\n",
    "\n",
    "        images.append(image)\n",
    "        labels.append(row[\"PDL1_score\"])\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# --- Create Datasets ---\n",
    "\n",
    "# Training dataset without augmentation\n",
    "X_train, y_train = load_dataset(train_df, image_folder, augment=False)\n",
    "\n",
    "# Training dataset with data augmentation\n",
    "X_train_aug, y_train_aug = load_dataset(train_df, image_folder, augment=True)\n",
    "\n",
    "# Validation dataset\n",
    "X_val, y_val = load_dataset(val_df, image_folder, augment=False)\n",
    "\n",
    "# Test dataset\n",
    "X_test, y_test = load_dataset(test_df, image_folder, augment=False)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9026830-64d9-4804-8410-bd530ba4b63e",
   "metadata": {},
   "source": [
    "Verifying Normalization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ea106-47bb-40bd-b507-4f7d5cce29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(X_train)) # Mean = 0\n",
    "print(np.std(X_train)) # Standard deviation = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12911c-d980-4782-95ca-d5f43cafde41",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcf693a-b734-49be-9fe1-f1931e3e3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= to_categorical(y_train)\n",
    "y_val= to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52950d6-decb-4009-9788-21fc4f22d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import plot_model\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "def identity_block(x, filter):\n",
    "    # copy tensor to variable called x_skip\n",
    "    x_skip = x\n",
    "    # Layer 1\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    # Layer 2\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    # Add Residue\n",
    "    x = tf.keras.layers.Add()([x, x_skip])     \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def convolutional_block(x, filter):\n",
    "    # copy tensor to variable called x_skip\n",
    "    x_skip = x\n",
    "    # Layer 1\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same', strides = (2,2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    # Layer 2\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    # Processing Residue with conv(1,1)\n",
    "    x_skip = tf.keras.layers.Conv2D(filter, (1,1), strides = (2,2))(x_skip)\n",
    "    # Add Residue\n",
    "    x = tf.keras.layers.Add()([x, x_skip])     \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def ResNet(shape = (32, 32, 3), classes = 10, block_layers = [3, 4, 6, 3]):\n",
    "    # Step 1 (Setup Input Layer)\n",
    "    x_input = tf.keras.layers.Input(shape)\n",
    "    x = tf.keras.layers.ZeroPadding2D((3, 3))(x_input)\n",
    "    # Step 2 (Initial Conv layer along with maxPool)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
    "    # Define size of sub-blocks and initial filter size\n",
    "    block_layers = block_layers\n",
    "    filter_size = 64\n",
    "    # Step 3 Add the Resnet Blocks\n",
    "    for i in range(4):\n",
    "        if i == 0:\n",
    "            # For sub-block 1 Residual/Convolutional block not needed\n",
    "            for j in range(block_layers[i]):\n",
    "                x = identity_block(x, filter_size)\n",
    "        else:\n",
    "            # One Residual/Convolutional Block followed by Identity blocks\n",
    "            # The filter size will go on increasing by a factor of 2\n",
    "            filter_size = filter_size*2\n",
    "            x = convolutional_block(x, filter_size)\n",
    "            for j in range(block_layers[i] - 1):\n",
    "                x = identity_block(x, filter_size)\n",
    "    # Step 4 End Dense Network\n",
    "    x = tf.keras.layers.AveragePooling2D((2,2), padding = 'same')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.Dense(classes, activation = 'softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs = x_input, outputs = x, name = \"ResNet34\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37f12233-96d4-4f7b-8b5a-7761e919cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROWS = 224\n",
    "# COLS = 224\n",
    "# CHANNELS = 46\n",
    "# CLASSES = 2\n",
    "# block_layers = [2,2,2,2]\n",
    "# # Build Network Graph \n",
    "# model_ResNet18 = ResNet(shape = (ROWS, COLS, CHANNELS), classes = CLASSES, block_layers = block_layers)\n",
    "\n",
    "# # Compile Model \n",
    "# l_rate = 1.e-4\n",
    "# opt = keras.optimizers.Adam(learning_rate=l_rate)\n",
    "\n",
    "# model_ResNet18.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # # Apply TensorBoard\n",
    "# # # define the logs folder \n",
    "# # log_dir = os.path.join(\"logs_ResNet50\", \"fit\", \"model_ResNet50_3Channels\" + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# # # Define TensorBoard Callback\n",
    "# # tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# # Train Model \n",
    "# batch = 32\n",
    "# epochs = 10\n",
    "# start_time = time.time()\n",
    "\n",
    "# history_ResNet18 = model_ResNet18.fit(X_train, y_train,\n",
    "#                                       epochs = epochs, batch_size = batch, \n",
    "#                                       validation_data = (X_val, y_val))\n",
    "\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"\\nElapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2604874e-48c8-47ac-a2d8-fa42d4d6725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ResNet18.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63325950-9d67-4663-8186-c7ef7564e94c",
   "metadata": {},
   "source": [
    "Finally, we have a Validation Accuracy above the baseline accuracy, giving us a proof of concept and giving us the ability to experiment further with: \n",
    "\n",
    "For now, we have 68.58% test accuracy\n",
    "\n",
    "    - Different ResNet Architecture: ResNet18, ResNet50, and pre-trained ResNets\n",
    "    - Data Augmentation\n",
    "    - Dimensionality Reduction: Autoencoder\n",
    "    - Feature Engineering: Channel Selection\n",
    "    - Regularizations: Regularizer and Dropouts\n",
    "    - Callbacks\n",
    "    - Monitor hyperparameters with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de0a6b-9698-4522-86ee-718be817ca9a",
   "metadata": {},
   "source": [
    "## ResNet18: with Tensor Board + 46 Channels + without Regularization + no callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc1ed662-89c2-48b4-9e82-8314b92e2e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 21:02:24.733828: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5447106560 exceeds 10% of free system memory.\n",
      "2024-12-15 21:02:30.556470: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5447106560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 21:02:44.753749: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.5743 - loss: 1.5904 - val_accuracy: 0.6396 - val_loss: 1.6029\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 574ms/step - accuracy: 0.7853 - loss: 0.5136 - val_accuracy: 0.6447 - val_loss: 0.9970\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 548ms/step - accuracy: 0.8780 - loss: 0.2858 - val_accuracy: 0.4010 - val_loss: 1.5503\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 555ms/step - accuracy: 0.9531 - loss: 0.1533 - val_accuracy: 0.6802 - val_loss: 0.7513\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 561ms/step - accuracy: 0.9848 - loss: 0.0689 - val_accuracy: 0.6701 - val_loss: 0.8414\n",
      "Epoch 6/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 562ms/step - accuracy: 0.9822 - loss: 0.0776 - val_accuracy: 0.6396 - val_loss: 1.0069\n",
      "Epoch 7/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 541ms/step - accuracy: 0.9925 - loss: 0.0457 - val_accuracy: 0.7157 - val_loss: 0.8810\n",
      "Epoch 8/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 573ms/step - accuracy: 1.0000 - loss: 0.0152 - val_accuracy: 0.6853 - val_loss: 0.8911\n",
      "Epoch 9/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 597ms/step - accuracy: 0.9986 - loss: 0.0057 - val_accuracy: 0.6954 - val_loss: 0.9259\n",
      "Epoch 10/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 575ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.6904 - val_loss: 1.0552\n",
      "\n",
      "Elapsed time: 109.94851922988892 seconds\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard \n",
    "ROWS = 224\n",
    "COLS = 224\n",
    "CHANNELS = 46\n",
    "CLASSES = 2\n",
    "block_layers = [2,2,2,2] # ResNet18\n",
    "# Build Network Graph \n",
    "model_ResNet18 = ResNet(shape = (ROWS, COLS, CHANNELS), classes = CLASSES, block_layers = block_layers)\n",
    "\n",
    "# Compile Model \n",
    "l_rate = 1.e-4\n",
    "opt = keras.optimizers.Adam(learning_rate=l_rate)\n",
    "\n",
    "model_ResNet18.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Apply TensorBoard\n",
    "# define the logs folder \n",
    "log_dir = os.path.join(\"logs_ResNet50\", \"fit\", \"model_ResNet18_46_no_regularization_trained\" + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# Define TensorBoard Callback\n",
    "tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Train Model \n",
    "batch = 64\n",
    "epochs = 10\n",
    "start_time = time.time()\n",
    "\n",
    "history_ResNet18 = model_ResNet18.fit(X_train, y_train,\n",
    "                                      epochs = epochs, batch_size = batch, \n",
    "                                      validation_data = (X_val, y_val), callbacks = tb_callback)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nElapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62b6fa15-0f72-49c1-a757-d5f621187d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.6562 - loss: 1.0567\n",
      "Test Accuracy is: 0.65%\n"
     ]
    }
   ],
   "source": [
    "#### Test accuracy\n",
    "print(\"Test Accuracy is: {:.2f}%\".format(model_ResNet18.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585dd86-5f33-4eb9-8931-56836dec602c",
   "metadata": {},
   "source": [
    "#### SAVING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4aebb33-b899-490f-bc92-dec3aaa29e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the model\n",
    "model_ResNet18.save('./models_ResNet/model_ResNet18_46_no_regularization_trained.keras')\n",
    "# Save the history of your experiments \n",
    "with open('./models_ResNet/history_red_LR.pkl', 'wb') as f:\n",
    "    pickle.dump(history_ResNet18.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ffe47c-ec7a-425a-bc33-fe919f827fa7",
   "metadata": {},
   "source": [
    "## ResNet18: with Tensor Board + 46 Channels + without Regularization + ReduceLR + EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "610e88cc-d3c6-40c2-98d1-f39c1638d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "log_dir = os.path.join(\"logs_ResNet50\", \"fit\", \"model_ResNet18_46_no_regularization_trained\" + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=12,   # Optimal patience value for validation accuracy\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"./models_ResNet/model_ResNet18_46_no_regularization_trained.keras\", # save validation loss into file\n",
    "        monitor=\"val_loss\",  # monitor validation accuracy\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,  \n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.001,\n",
    "    min_lr=1.e-6,\n",
    "    ), \n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f78d2f-35e0-4636-bc5c-1aa0535feb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 21:08:32.109724: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5447106560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.6366 - loss: 1.5402 - val_accuracy: 0.3604 - val_loss: 2.5013 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 716ms/step - accuracy: 0.8172 - loss: 0.5057 - val_accuracy: 0.4721 - val_loss: 0.9120 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 584ms/step - accuracy: 0.9285 - loss: 0.2298 - val_accuracy: 0.4822 - val_loss: 1.1409 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 720ms/step - accuracy: 0.9554 - loss: 0.1310 - val_accuracy: 0.6802 - val_loss: 0.7067 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 591ms/step - accuracy: 0.9975 - loss: 0.0662 - val_accuracy: 0.6447 - val_loss: 0.7182 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 771ms/step - accuracy: 0.9940 - loss: 0.0332 - val_accuracy: 0.7056 - val_loss: 0.6775 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 580ms/step - accuracy: 0.9978 - loss: 0.0193 - val_accuracy: 0.6954 - val_loss: 0.6818 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 759ms/step - accuracy: 1.0000 - loss: 0.0123 - val_accuracy: 0.7107 - val_loss: 0.6763 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 603ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.7107 - val_loss: 0.7336 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 605ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.7208 - val_loss: 0.6787 - learning_rate: 1.0000e-04\n",
      "\n",
      "Elapsed time: 110.09941244125366 seconds\n"
     ]
    }
   ],
   "source": [
    "ROWS = 224\n",
    "COLS = 224\n",
    "CHANNELS = 46\n",
    "CLASSES = 2\n",
    "block_layers = [2,2,2,2] # ResNet18\n",
    "# Build Network Graph \n",
    "model_ResNet18 = ResNet(shape = (ROWS, COLS, CHANNELS), classes = CLASSES, block_layers = block_layers)\n",
    "\n",
    "# Compile Model \n",
    "l_rate = 1.e-4\n",
    "opt = keras.optimizers.Adam(learning_rate=l_rate)\n",
    "\n",
    "model_ResNet18.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Apply TensorBoard\n",
    "# define the logs folder \n",
    "log_dir = os.path.join(\"logs_ResNet50\", \"fit\", \"model_ResNet18_46_no_regularization_trained\" + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# Define TensorBoard Callback\n",
    "tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Train Model \n",
    "batch = 64\n",
    "epochs = 10\n",
    "start_time = time.time()\n",
    "\n",
    "history_ResNet18 = model_ResNet18.fit(X_train, y_train,\n",
    "                                      epochs = epochs, batch_size = batch, \n",
    "                                      validation_data = (X_val, y_val), callbacks = callbacks_list)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nElapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feaef6b7-943e-40fa-ad2c-243ae7c2f09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.7067 - loss: 0.9014\n",
      "Test Accuracy is: 0.71%\n"
     ]
    }
   ],
   "source": [
    "#### Test accuracy\n",
    "print(\"Test Accuracy is: {:.2f}%\".format(model_ResNet18.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350440f3-3a72-44eb-94bf-83fe1fda4343",
   "metadata": {},
   "source": [
    "#### COMMENTS\n",
    "\n",
    "Test accuracy improved a little bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9438e52-ae23-4c99-b25f-4834b8cb93f7",
   "metadata": {},
   "source": [
    "#### SAVING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d773db4-2ff8-428c-a301-e5a1e52ef3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the model\n",
    "model_ResNet18.save('./models_ResNet/model_ResNet18_46_no_regularization_callbacks_trained.keras')\n",
    "# Save the history of your experiments \n",
    "with open('./models_ResNet/history_ResNet18_46_no_regularization_callbacks_trained.pkl', 'wb') as f:\n",
    "    pickle.dump(history_ResNet18.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7306615f-7fbe-4702-97ac-7cec5f051fb3",
   "metadata": {},
   "source": [
    "## ResNet18: with Tensor Board + 46 Channels + with Dropout + ReduceLR + EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16897799-090b-4b2b-b2d7-edcc7d925976",
   "metadata": {},
   "source": [
    "### Redefine Network with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44db92f7-d88f-4b3b-b39d-37fa28583602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(x, filter, dropout_rate = 0.5):\n",
    "    # copy tensor to variable called x_skip\n",
    "    x_skip = x\n",
    "    # Layer 1\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    # Layer 2\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    # Add Residue\n",
    "    x = tf.keras.layers.Add()([x, x_skip])     \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def convolutional_block(x, filter, dropout_rate = 0.5):\n",
    "    # copy tensor to variable called x_skip\n",
    "    x_skip = x\n",
    "    # Layer 1\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same', strides = (2,2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    # Layer 2\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    # Processing Residue with conv(1,1)\n",
    "    x_skip = tf.keras.layers.Conv2D(filter, (1,1), strides = (2,2))(x_skip)\n",
    "    # Add Residue\n",
    "    x = tf.keras.layers.Add()([x, x_skip])     \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def ResNet(shape = (32, 32, 3), classes = 10, block_layers = [3, 4, 6, 3], dropout_rate = 0.5):\n",
    "    # Step 1 (Setup Input Layer)\n",
    "    x_input = tf.keras.layers.Input(shape)\n",
    "    x = tf.keras.layers.ZeroPadding2D((3, 3))(x_input)\n",
    "    # Step 2 (Initial Conv layer along with maxPool)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
    "    # Define size of sub-blocks and initial filter size\n",
    "    block_layers = block_layers\n",
    "    filter_size = 64\n",
    "    # Step 3 Add the Resnet Blocks\n",
    "    for i in range(4):\n",
    "        if i == 0:\n",
    "            # For sub-block 1 Residual/Convolutional block not needed\n",
    "            for j in range(block_layers[i]):\n",
    "                x = identity_block(x, filter_size)\n",
    "        else:\n",
    "            # One Residual/Convolutional Block followed by Identity blocks\n",
    "            # The filter size will go on increasing by a factor of 2\n",
    "            filter_size = filter_size*2\n",
    "            x = convolutional_block(x, filter_size)\n",
    "            for j in range(block_layers[i] - 1):\n",
    "                x = identity_block(x, filter_size)\n",
    "    # Step 4 End Dense Network\n",
    "    x = tf.keras.layers.AveragePooling2D((2,2), padding = 'same')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.Dense(classes, activation = 'softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs = x_input, outputs = x, name = \"ResNet34\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42af721f-0a70-43e3-9aa8-12ddf1562b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "log_dir = os.path.join(\"logs_ResNet50\", \"fit\", \"model_ResNet18_46_dropout_callbacks_trained\" + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=12,   # Optimal patience value for validation accuracy\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"./models_ResNet/model_ResNet18_46_dropout_callbacks_trained.keras\", # save validation loss into file\n",
    "        monitor=\"val_loss\",  # monitor validation accuracy\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,  \n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.001,\n",
    "    min_lr=1.e-6,\n",
    "    ), \n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4396d29c-0615-45f1-acb5-709b284c89c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 21:24:09.128733: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5447106560 exceeds 10% of free system memory.\n",
      "2024-12-15 21:24:16.389368: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5447106560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - accuracy: 0.6218 - loss: 1.4648 - val_accuracy: 0.3655 - val_loss: 1.6273 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 628ms/step - accuracy: 0.6907 - loss: 0.8077 - val_accuracy: 0.3858 - val_loss: 1.1247 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 618ms/step - accuracy: 0.6596 - loss: 0.6948 - val_accuracy: 0.4213 - val_loss: 0.8410 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 627ms/step - accuracy: 0.6950 - loss: 0.6337 - val_accuracy: 0.4975 - val_loss: 0.7305 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 625ms/step - accuracy: 0.7115 - loss: 0.5754 - val_accuracy: 0.4924 - val_loss: 0.8391 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 615ms/step - accuracy: 0.8052 - loss: 0.4526 - val_accuracy: 0.5838 - val_loss: 0.7443 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.8025 - loss: 0.4379\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 594ms/step - accuracy: 0.8042 - loss: 0.4352 - val_accuracy: 0.6497 - val_loss: 0.7474 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 596ms/step - accuracy: 0.8372 - loss: 0.3764 - val_accuracy: 0.7005 - val_loss: 0.6426 - learning_rate: 5.0000e-05\n",
      "Epoch 9/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 625ms/step - accuracy: 0.8683 - loss: 0.3485 - val_accuracy: 0.7513 - val_loss: 0.5732 - learning_rate: 5.0000e-05\n",
      "Epoch 10/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 617ms/step - accuracy: 0.8804 - loss: 0.3056 - val_accuracy: 0.7513 - val_loss: 0.5707 - learning_rate: 5.0000e-05\n",
      "Epoch 11/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 617ms/step - accuracy: 0.8839 - loss: 0.3057 - val_accuracy: 0.8071 - val_loss: 0.5211 - learning_rate: 5.0000e-05\n",
      "Epoch 12/30\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 639ms/step - accuracy: 0.8853 - loss: 0.2663 - val_accuracy: 0.7970 - val_loss: 0.5281 - learning_rate: 5.0000e-05\n",
      "\n",
      "Elapsed time: 140.72206950187683 seconds\n"
     ]
    }
   ],
   "source": [
    "ROWS = 224\n",
    "COLS = 224\n",
    "CHANNELS = 46\n",
    "CLASSES = 2\n",
    "block_layers = [2,2,2,2] # ResNet18\n",
    "dropout_rate = 0.5\n",
    "# Build Network Graph \n",
    "model_ResNet18 = ResNet(shape = (ROWS, COLS, CHANNELS), classes = CLASSES, block_layers = block_layers, dropout_rate = dropout_rate)\n",
    "\n",
    "# Compile Model \n",
    "l_rate = 1.e-4\n",
    "opt = keras.optimizers.Adam(learning_rate=l_rate)\n",
    "\n",
    "model_ResNet18.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Apply TensorBoard\n",
    "# define the logs folder \n",
    "log_dir = os.path.join(\"logs_ResNet50\", \"fit\", \"model_ResNet18_46_dropout_callbacks_trained\" + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# Define TensorBoard Callback\n",
    "tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Train Model \n",
    "batch = 64\n",
    "epochs = 30\n",
    "start_time = time.time()\n",
    "\n",
    "history_ResNet18 = model_ResNet18.fit(X_train, y_train,\n",
    "                                      epochs = epochs, batch_size = batch, \n",
    "                                      validation_data = (X_val, y_val), callbacks = callbacks_list)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nElapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11414950-e805-4a70-bec6-12f0077d6ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - accuracy: 0.7751 - loss: 0.6151\n",
      "Test Accuracy is: 0.78%\n"
     ]
    }
   ],
   "source": [
    "#### Test accuracy\n",
    "print(\"Test Accuracy is: {:.2f}%\".format(model_ResNet18.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd9f7a-de72-4640-9bc2-05aeb0c96db7",
   "metadata": {},
   "source": [
    "#### SAVING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09c1b7-69e0-4f57-b20d-9cbc8b31d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the model\n",
    "model_ResNet18.save('./models_ResNet/model_ResNet18_46_dropout_callbacks_trained.keras')\n",
    "# Save the history of your experiments \n",
    "with open('./models_ResNet/model_ResNet18_46_dropout_callbacks_trained.pkl', 'wb') as f:\n",
    "    pickle.dump(history_ResNet18.history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BME342 Env",
   "language": "python",
   "name": "bme342_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
